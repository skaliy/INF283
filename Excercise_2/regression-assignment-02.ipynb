{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF283 | Weekly Exercise 02 | Regression\n",
    "\n",
    "#### Deadline\n",
    "Sep 07, 2018 | 23:59\n",
    "\n",
    "#### What to deliver \n",
    "You can try out your Python code within this notebook. But you should make a PDF file of answers for each of the tasks, and then submit this PDF file on Mitt UiB.\n",
    "\n",
    "#### Where to deliver\n",
    "On [Mitt UiB](https://mitt.uib.no/courses/12791/assignments) in the assignments section.\n",
    "\n",
    "#### Note to students\n",
    "This is a Python notebook and all the examples are in Python. You are free to use any programming language you want to do the exercise questions in; we will grade your submissions. But, if you got stuck somewhere, we can provide help for Python only. Moreover, the solution provided after the deadline will also in be in Python.\n",
    "\n",
    "In case you want to learn Python, you can quickly learn it using these [set of short videos](https://www.youtube.com/watch?v=oVp1vrfL_w4&list=PLQVvvaa0QuDe8XSftW-RAxdo6OmaeL85M)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "[1. Univariate Linear Regression](#univariate)\n",
    "  * [Exercise 1.1](#exercise1_1)\n",
    "\n",
    "  * [Exercise 1.2](#exercise1_2)\n",
    "\n",
    "  * [Exercise 1.3](#exercise1_3)\n",
    "\n",
    "  * [Exercise 1.4](#exercise1_4)\n",
    "\n",
    "  * [Exercise 1.5](#exercise1_5)\n",
    "\n",
    "[2. Multivariate Linear Regression](#multivariate)\n",
    "  \n",
    "  * [Exercise 2.1](#exercise2_1)\n",
    "  \n",
    "  * [Exercise 2.2](#exercise2_2)\n",
    "  \n",
    "  * [Exercise 2.3](#exercise2_3)\n",
    "\n",
    "[3. Logistic Regression](#logistic)\n",
    "\n",
    "  * [Exercise 3.1](#exercise3_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Univariate Linear Regression <a class=\"anchor\" id=\"univariate\"></a> \n",
    "In univariate regression, the response variable is modeled in terms of just one predictor variable. In this section, we will experiment with various ways in which we can perform linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Gradient Descent\n",
    "\n",
    "In the lecture, you saw the closed-form solution \n",
    "$$\\hat{w} = (X^TX)^{-1}X^Ty$$\n",
    "to find the best values of linear regression parameters.\n",
    "This closed-form solution exists only when the matrix X is invertible. Even if the matrix X is invertible, but is very large in its dimensions, it might not be possible to invert it with the compute resources that you might have at your disposal. In these scenarios, we can use gradient descent to find the best value of parameter estimates.\n",
    "\n",
    "Gradient descent attempts to find the *best* values for these parameters so that the value of some error function is minimized. We will be using Mean Squared Error (MSE) as our error function, which is the mean of the sum of squared error (SSE). Error functions are also called the loss functions.\n",
    "\n",
    "This code demonstrates how a gradient descent search may be used to solve the linear regression problem of fitting a line through a set of points. The goal is to model a set of points with a straight line. A straight line is defined by two parameters: the line's slope $w_0$ , and its y-intercept $w_1$ (some texts may denote $w_0$ and $w_1$ as $b$ and $m$, respectively, so don't get confused by the notation). \n",
    "\n",
    "$w_0$ and $w_1$ can take on any value, but only certain specific values of $w_0$ and $w_1$ will yield a line that minimizes the sum of squares error between the original points and the line that tries to model these points (refer to this [interactive demo](http://www.dangoldstein.com/regression.html) to get a better understanding of this concept).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install numpy first. If you have it installed already then pip won't install it\n",
    "# and you will get requirement already satisfied message. That's normal.\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use numpy, we must first import it\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the mean squared error (MSE)\n",
    "Now we define a function called `compute_error_for_line_given_points` to find the mean squared error between a line (modeled by parameters `w0` and `w1`) and a set of points to which it tries to model. Mathematically, it is given as:\n",
    "$$\n",
    "MSE = \\dfrac{1}{n} \\sum_{i=1}^{n} [y_i - (w_0 x_i + w_1)]^2 \n",
    "$$\n",
    "where `n` is the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now going to define a function which takes the parameters (b and m) of \n",
    "# a line and then finds the mean-squared error between the user-specified points \n",
    "# and the line. \n",
    "def compute_error_for_line_given_points(w0, w1, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # accumulate 'sum of square' errors in totalError variable\n",
    "        totalError += (y - (w1 * x + w0)) ** 2\n",
    "    # find mean of sum of squared errors    \n",
    "    mse = totalError/len(points)\n",
    "    return mse\n",
    "\n",
    "# N.B.: Students who wish to do this excercise in R should implement this function in R themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "In gradient descent, we start with some initial values of $w_0$ and $w_1$ and then  refine these crude estimate until we can no longer see an appreciable decrease in the mean squared error. To refine the $w_0$ and $w_1$ estimates, we need to update them (make their values higher or lower) so that the mean squared error gets reduced. Taking partial derivative of MSE with respect to $w_0$ and $w_1$, can tells us whether to increase or decrease the values of these parameter to get an improved fit.\n",
    "\n",
    "\n",
    "If we take the partial derivative of the MSE function with respect of $w_0$, we get:\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial w_0} =\\frac{\\partial}{\\partial w_0} \\left(\\dfrac{1}{n} \\sum_{i=1}^{n} [y_i - (w_1 x_i + w_0)]^2\\right)\n",
    "=-\\dfrac{2}{n}\\left( \\sum_{i=1}^{n} y_i - (w_1 x_i + w_0)\\right)\n",
    "$$\n",
    "\n",
    "If we take the partial derivative of the MSE function with respect of $w_1$, we get:\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial w_1} =\\frac{\\partial}{\\partial w_1} \\left(\\dfrac{1}{n} \\sum_{i=1}^{n} [y_i - (w_1 x_i + w_0)]^2\\right) \n",
    "=-\\dfrac{2}{n}\\left( \\sum_{i=1}^{n} x_i[y_i - (w_1 x_i + w_0)]\\right)\n",
    "$$\n",
    "\n",
    "The gradient $\\nabla$ of MSE is just as a vector of these partial derivatives of MSE with respect to $w_0$ and $w_0$:\n",
    "$$\n",
    "\\nabla MSE(w_0, w_1) = \\left(\\frac{\\partial MSE}{\\partial w_0}, \\frac{\\partial MSE}{\\partial w_1}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "We can now define the update rule for $w_0$ and $w_1$. Update rule modify the current value of the parameter such that the updated parameter values cause a decrease in the MSE.\n",
    "\n",
    "Update rule for the weight vector of $w_0$ and $w_1$:\n",
    "$$\n",
    "(w_0, w_1)_{t+1} =(w_0, w_1)_{t} - \\eta \\left(\\frac{\\partial MSE}{\\partial w_0}, \\frac{\\partial MSE}{\\partial w_1}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "The `step_gradient` function below shows you how to implement the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(w0_current, w1_current, points, learningRate):\n",
    "    #initialize the partial derivatives for the cummlative sum\n",
    "    w0_par_der = 0\n",
    "    w1_par_der = 0\n",
    "    \n",
    "    n = len(points)\n",
    "    \n",
    "    # computation for the summation\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # partial derivative (of MSE) with respect to w0\n",
    "        w0_par_der += (y - ((w1_current * x) + w0_current))\n",
    "        # partial derivative (of MSE) with respect to w1\n",
    "        w1_par_der += x * (y - ((w1_current * x) + w0_current))\n",
    "        \n",
    "    # multiplcation of summation results with -2/n\n",
    "    w0_par_der = -(2/n) * w0_par_der\n",
    "        # partial derivative (of MSE) with respect to w1\n",
    "    w1_par_der = -(2/n) * w1_par_der\n",
    "         \n",
    "    # make a gradient vector from the partial derivatives    \n",
    "    gradient_mse = np.array([w0_par_der, w1_par_der])\n",
    "    \n",
    "    # make a vector of weights\n",
    "    weight_vector = np.array([w0_current, w1_current])\n",
    "    \n",
    "    # update rule for weights\n",
    "    updated_weight_vector = weight_vector - (learningRate * gradient_mse)\n",
    "    \n",
    "    # return the updated weight vector as a list\n",
    "    return np.ndarray.tolist(updated_weight_vector)\n",
    "\n",
    "# N.B. Students who wish to do this excercise in R should implement this function in R themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Gradient Descent Iteratively\n",
    "Gradient descent is an iterative method of improving the parameter estimates. Therefore, we run `step_gradient` function until we reach the absolute minimum of error, or until we have executed the `gradient_step` function a certain number of times, or if the error is small. Here we implement a function called `gradient_descent_runner` that runs the gradient_step function for `num_iterations` times. In the exercise later on, we will ask you to make some changes in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner(points, starting_w0, starting_w1, learning_rate, num_iterations):\n",
    "    w0 = starting_w0\n",
    "    w1 = starting_w1\n",
    "    for i in range(num_iterations):\n",
    "        w0, w1 = step_gradient(w0, w1, points, learning_rate)\n",
    "        mse = compute_error_for_line_given_points(w0, w1, points)\n",
    "        print(f'Iteration {i+1}: w0={w0:0.5f}, w1={w1:0.5f}, mse={mse:0.5f}')\n",
    "    return [w0, w1, mse]\n",
    "# N.B.: Students who wish to do this exercise in R should implement this function in R themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together\n",
    "All the functions we have define above won't do anything unless we call them. But we first need the data. We will generate the data points from a straight line and then add noise to it: \n",
    "$$\n",
    "y = 4 + 3x + noise\n",
    "$$\n",
    "The y-intercept ($w_0$) of the line of the line is 4, and the slope ($w_1$) of the line is 3.\n",
    "\n",
    "We will then use gradient descent to estimate the parameters of the line that generated this noisy linear data. If everything works correctly, then the parameters estimates should be close to the actual values of $w_0$ and $w_1$ used to generate the original data.\n",
    "\n",
    "The first column is the x values, and the second column contains the y-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "# generate 100 x values from 0 to 2 randomly, then sort them in ascending order\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "X.sort(axis=0)\n",
    "\n",
    "# generate y values and add noise to it\n",
    "y = 4 + 3 * X + np.random.rand(100, 1)\n",
    "\n",
    "# let us plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the x and y values into a single array called points\n",
    "points = np.column_stack((X, y))\n",
    "\n",
    "num_iterations = 20\n",
    "learning_rate = 0.1\n",
    "initial_w0 = 0 # initial y-intercept guess\n",
    "initial_w1 = 0 # initial slope guess\n",
    "[w0, w1, mse] = gradient_descent_runner(points, initial_w0, initial_w1, learning_rate, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1 <a class=\"anchor\" id=\"exercise1_1\"></a> \n",
    "As you can see, the mean squared error decreases every iteration of gradient descent. Currently the gradient descent runs for 20 iterations. Making changes in the above code so that it now runs for 100 iterations. Observing the mean squared error now. You will see that the mean squared error stops decreasing any further by an appreciable amount after 50 iterations. But since you have specified that the gradient descent should run for 100 iterations, therefore, the program will run for 100 iterations, although the last 50 iterations would be pretty much useless and a waste of your compute resources. In our case, we have a very small dataset, and each iteration takes a very small amount of time, but real-world datasets can be huge, and each iteration of gradient descent will take a considerable amount of time. So you can't afford to run gradient descent if it no longer yields a decrease in the loss.\n",
    "\n",
    "Your task is to now modify `gradient_descent_runner` function to implement an early stop such that if the improvement in error loss between two consecutive iterations of gradient descent steps is less than a certain user-specified threshold then the algorithm stops, otherwise the algorithm runs for a maximum of `num_iterations`. Name your function as `gradient_descent_runner_early_stop`\n",
    "\n",
    "Paste the code for modified `gradient_descent_runner_early_stop` function in the block provided below (the code can be Python or R):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "# Paste your code below for the modified gradient_descent_runner_early_stop function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2<a class=\"anchor\" id=\"exercise1_2\"></a> \n",
    "In the program above we had set the learning rate to 0.1. Using the original `gradient_descent_runner` function, first set the number of iterations to 100. Then try to run the code with two different values of learning rates:\n",
    "1. a learning rate of 0.001\n",
    "2. a learning rate of 1\n",
    "\n",
    "Explain what you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO\n",
    "Write you answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.3<a class=\"anchor\" id=\"exercise1_3\"></a> \n",
    "Earlier in the document, you learned about the closed-form solution of linear regression:\n",
    "$$\n",
    "\\hat{w} = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "where y is a vector of output values, and X is matrix of input vectors. It is important to note that the y-intercept ($w_0$) is represented by a column of 1's in the X matrix. \n",
    "\n",
    "This equation is called the Normal equation, and it can work only if the columns of the matrix X are linear independent, or in other words, the matrix X is invertible.\n",
    "\n",
    "Once you solve the normal equation, the vector $\\bar{w}$ will contain the parameter estimates ($w_0$ and $w_1$) of the line.\n",
    "\n",
    "Your task is to implement the normal equation, and give it the same values of `X` and `y` as we have used above,\n",
    "and see what the matrix $\\bar{w}$ contains. It should contain the parameter estimates of the fit.\n",
    "\n",
    "How do the parameter estimates obtained by solving the normal equation compare to the ones obtained using the gradient descent algorithm?\n",
    "\n",
    "##### Hints: \n",
    "1. Add a column of 1's to X to using `np.hstack`. This column of 1's models for the y-intercept ($w_0$). \n",
    "2. Use `inv` function in numpy to find the inverse of a matrix. inv is defined in `numpy.linalg` module so will need to import it from it. \n",
    "3. To multiply two matrices `a` and `b`, use the following notation in python `a.dot(b)`\n",
    "4. To transpose a matrix `c` use `c.T`\n",
    "\n",
    "Students who wish to do this in R instead of Python would find [this resource](https://www.statmethods.net/advstats/matrix.html) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.4<a class=\"anchor\" id=\"exercise1_4\"></a> \n",
    "To brush up your calculus skills, derive the partial derivate of MSE that has *L2* penalty term included in it. In other words, we want you compute the following partial derivatives:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_0} \\left(\\dfrac{1}{n} \\sum_{i=1}^{n} [y_i - (w_1 x_i + w_0)]^2 + \\lambda {w_0}^2\\right) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_1} \\left(\\dfrac{1}{n} \\sum_{i=1}^{n} [y_i - (w_1 x_i + w_0)]^2 + \\lambda {w_1}^2\\right) \n",
    "$$\n",
    "\n",
    "#### What to submit\n",
    "A derivation of both the gradient equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with *sklearn* Machine Learning Library\n",
    "In the section above, we implemented our own linear regression with gradient descent. However, instead of re-inventing the wheel and writing the code for each machine learning algorithm ourselves, we can also follow the easier path of using someone else's code to do machine learning. That's the path that most people follow. Having said that, knowing how a particular algorithm works internally is a very important for understanding its nuances.\n",
    "\n",
    "In this section, we will use a machine learning library called sklearn to perform linear regression on the same dataset and see what kind of results we get this time round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install sklearn if it isn't installed already \n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LinearRegression class from sklearn.linear_model module\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# make a lin_reg object form the LinearRegression class\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# use the fit method of LinearRegression class to fit a straight line through the data\n",
    "lin_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y-intercept w0:', lin_reg.intercept_)\n",
    "print('slope w1:', lin_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the estimates for $w_0$ and $w_1$ are very close to the estimates we obtained by using our gradient descent algorithm. Now lets plot the original data along with the fitted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the original data points as a scatter plot\n",
    "plt.scatter(X, y, label='original data')\n",
    "\n",
    "# plot the line that fits these points. Use the values of m and b as provided by the fit method\n",
    "y_ = lin_reg.coef_*X + lin_reg.intercept_\n",
    "\n",
    "# you can also get y_ by using the predict method. Uncomment the line below:\n",
    "#y_ = lin_reg.predict(X)\n",
    "\n",
    "plt.plot(X, y_, color='r', label='predicted fit')\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.5<a class=\"anchor\" id=\"exercise1_5\"></a> \n",
    "1. How does fit you got from sklearn library compare to the one you got form your implementation of the Normal equation.\n",
    "\n",
    "2. Suppose you have a new data point `x=3`. \n",
    "Use the `predict` method provided by the `LinearRegression` class to find its corresponding y value. \n",
    "\n",
    "N.B. Student who prefer R can implement linear regression using `lm()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# write you solution below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multivariate Linear Regression <a class=\"anchor\" id=\"multivariate\"></a> \n",
    "\n",
    "So far we we were using univariate linear regression. Let us now discuss multivariate linear regression. In multivariate regression, the dependent variable is modeled as a linear combination of multiple independent variables. \n",
    "\n",
    "To study multivariate regression, we are going to use the Hollywood movies dataset. This dataset is in `movies.csv` file. It has four columns and their description is as following:\n",
    "\n",
    "- `revenue` = Total revenue obtained in the first year of box office release in millions\n",
    "- `production_cost` = Total cost in million in producing the movie\n",
    "- `promotional_cost` = Total cost in millions in promoting the movies\t\n",
    "- `book_sales` = Total sales in millions of the movie's book\n",
    "\n",
    "We will now try to model the revenue as a linear combination of the production_cost, promotional_cost, and book_sales using Multivariate Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# make a dataframe of the data\n",
    "df = pd.read_csv('movies.csv')\n",
    "\n",
    "# show first five rows of df\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first column and set it to the output or dependent varaible y\n",
    "y = df[['revenue']]\n",
    "\n",
    "# Remove the first column and set the rest of the dataframe to X. This is the set of indepedent variables\n",
    "X = df.drop(columns=['revenue'])\n",
    "\n",
    "# show first five rows of X\n",
    "X.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first five rows of y\n",
    "y.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will fit a Multivariate Linear Regression model to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# make a lin_reg object form the LinearRegression class\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# use the fit method of LinearRegression class to fit a straight line through the data\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "# Display the learned parameters\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1<a class=\"anchor\" id=\"exercise2_1\"></a> \n",
    "What would be the first year box office revenue of a movie which costed 23 million dollars to make, 12 million dollars to promote, and had total book sales of 10 million dollars.\n",
    "\n",
    "N.B: Students you want to attempt this exercise in R can use `lm()` function to do multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "## Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Regression with Polynomial basis\n",
    "What if your data is actually more complex than a simple straight line, or a plane? Surprisingly, you can actually use a linear model to fit nonlinear data as well. A simple way to do this is to add powers of each feature as new\n",
    "features, and then train a linear model on this extended set of features. This technique is called Polynomial\n",
    "Regression. Because we are modeling the response variable as linear combination of the polynomial features, therefore this regression is classed as multivariate regression.\n",
    "\n",
    "Lets look at an example. First, lets generate some nonlinear data, based on a simple quadratic equation (plus some noise).\n",
    "$$\n",
    "y = 0.5X^2 + X + 2 + noise\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of points to generate as k\n",
    "k = 100\n",
    "\n",
    "# define a seed value. It is important to define the seed value\n",
    "# so that the random numbers generated are the same every time\n",
    "# this code is executed.\n",
    "np.random.seed(10)\n",
    "\n",
    "# generate k x-axis values from -3 to +3\n",
    "X = 6 * np.random.rand(k, 1) - 3\n",
    "\n",
    "# sort the numbers in ascending order. This helps when we are plotting the data. \n",
    "# Without this line, your plots will be all jumbled up\n",
    "X.sort(axis=0)\n",
    "\n",
    "# generate k y-axis values\n",
    "y = 0.5 * X**2 + X + 2 + np.random.rand(k, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.xlabel('x'); plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, a straight line will never fit this data properly. So lets use Scikit-Learn’s `PolynomialFeatures`\n",
    "class to transform our training data by adding the square of X in the training set as new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# generate polyonimal features upto degree 2 from the vector X\n",
    "X_poly = poly_features.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display 4 original data points \n",
    "X[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the transformed data.\n",
    "# You will now see the original X data alongside its corresponding 2nd-degree polynomial feature\n",
    "X_poly[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit a linear regression model to the transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Now we fit a linear model to the X_poly (the transformed features set) and y\n",
    "lin_reg.fit(X_poly, y)\n",
    "\n",
    "# show the values of intercept and learned co-efficients\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the fit estimated by polynomial regression has the following form:\n",
    "\n",
    "$$\n",
    "\\bar{y} = 0.49X^2 + 0.99X + 2.48\n",
    "$$\n",
    "\n",
    "which is pretty close the function we used to generate the original the original data:\n",
    "$$\n",
    "y = 0.5X^2 + X + 2 + noise\n",
    "$$\n",
    "\n",
    "(N.B.: The values that you might get for `lin_reg.intercept_` and `lin_reg.coef_` may be a bit different. This is because you might be using a different computer with a different operating system, different precision etc. All this might lead a slightly different values of intercept and co-efficients.)\n",
    "\n",
    "Let's now plot the original data (in blue) and the predicted fit (in red):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_ = lin_reg.predict(X_poly)\n",
    "    \n",
    "plt.scatter(X, y, label='original data')\n",
    "plt.plot(X, y_, color='r', label='predicted fit')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try it yourself\n",
    "Change the value of polynomial degree, and see what happens to the fitted line.\n",
    "\n",
    "Try values of 5, 10, and 15. You don't need to submit anything for this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization with Ridge Penalty\n",
    "\n",
    "As you might have noticed from the above task, as the degree of polynomial increases, the model starts to overfit. Model overfitting often happens when you have:\n",
    "- lots of features, and \n",
    "- too little data per feature. \n",
    "\n",
    "The model starts memorizing the data, rather than generalizing.\n",
    "\n",
    "To prevent model from overfitting, we can do regularization. In regularization, we add a penalty term to the loss function. This penalty term prevents the model from overfitting.\n",
    "\n",
    "Here, we will write the code for Ridge regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# define the number of points to generate\n",
    "k = 100\n",
    "np.random.seed(10)\n",
    "\n",
    "# generate k x-axis values from -3 to +3\n",
    "X = 6 * np.random.rand(k, 1) - 3\n",
    "X.sort(axis=0)\n",
    "\n",
    "# generate k y-axis values\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(k, 1)\n",
    "\n",
    "# Create polynomial feature (degree 15)\n",
    "poly_features = PolynomialFeatures(degree=15, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "# Create Ridge regression object from Ridge class\n",
    "ridge_reg = Ridge(alpha=5)\n",
    "# Fit data using Ridge regression\n",
    "ridge_reg.fit(X_poly, y)\n",
    "\n",
    "# Create Linear regression object from LinearRegress class (this is just for comparison)\n",
    "lin_reg = LinearRegression()\n",
    "# Fit data using Linear regression\n",
    "lin_reg.fit(X_poly, y)\n",
    "\n",
    "y_predict_ridge = ridge_reg.predict(X_poly)\n",
    "y_predict_linear = lin_reg.predict(X_poly)\n",
    "\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.plot(X, y_predict_linear, color='b', label='Linear regression fit')\n",
    "plt.plot(X, y_predict_ridge, color='r', label='Ridge regression fit')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the plot above, linear regression leads to overfitting (jiggly blue fit in the plot above).\n",
    "\n",
    "Ridge regression with `alpha=5` yields a fit that isn't overfitted (smooth red curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2<a class=\"anchor\" id=\"exercise2_2\"></a> \n",
    "Change the value of regularization parameter to 0.05 and see what happens. Explain what you observe?\n",
    "\n",
    "N.B. Students who wish to do this exercise in R can use the `lm.ridge()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO\n",
    "Write you answer here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Radial Basis Functions\n",
    "In the lecture, you learned about the radial basis functions (RBF). A radial basis function is a real-valued function whose value depends only on the distance from some point `c` called a center. We can use each point of our data as center of an RBF and then use a weighted sums of these radial basis functions to approximate the fit for the original data. In this case multivariate linear regression can be used to find the weights (or coefficients) of this weighted sum of `m` radial basis functions, where `m` are the number of data points in the dataset for which we want to find a fit.\n",
    "\n",
    "Here you will see an implementation of multivariate linear regression with radial basis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "m = 100\n",
    "\n",
    "# Create random set of m x values between -6 and +6\n",
    "X = np.random.rand(m, 1)*12 - 6\n",
    "X.sort(axis =0)\n",
    "\n",
    "# Create a non-linear dataset with random noise\n",
    "y = 0.5*np.cos(X) + np.sin(X) + 4*np.cos(2*X) + np.exp(np.cos(3*X)) + 3*np.random.rand(m,1) \n",
    "\n",
    "# plot it\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# find the transformation of X using Radial Basis Functions \n",
    "# Each point in X is now modeled as vector of 100 values. \n",
    "# See the X_RBF.shape and X_RBF to find how rbf_kernel transformed \n",
    "# the original datapoints\n",
    "X_RBF = rbf_kernel(X, X, gamma=0.1) \n",
    "\n",
    "# Fit a linear regression model to the RBF-transformed data\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_RBF, y)\n",
    "\n",
    "# find the predicted values\n",
    "y_= clf.predict(X_RBF)\n",
    "\n",
    "# plot original data and predicted fit\n",
    "plt.scatter(X, y, label='Original data')\n",
    "plt.plot(X, y_, color='r', label='Fit for RBF-transformed data')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3<a class=\"anchor\" id=\"exercise2_3\"></a> \n",
    "Plot the first 0th, 99th, and 49th radial basis of X on the same plot as a line graph.\n",
    "\n",
    "##### HINT:\n",
    "X_RBF contains 100 radial basis functions, corresponding to each of the 100 data points. All you need to do is to index them, and then plot them.\n",
    "\n",
    "N.B.: Students who wish to do this exercise in R should refer to this [resource for RBF](http://www.di.fc.ul.pt/~jpn/r/rbf/rbf.html).\n",
    "\n",
    "Paste your code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Paste your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression <a class=\"anchor\" id=\"logistic\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression (also called Logit Regression) is a generalized linear model which is commonly used to estimate the probability that an instance belongs to a particular class (e.g., what is the probability that this email is spam?). If the estimated probability is greater than 0.5, then the model predicts that the instance belongs to that class (called the positive class, labeled “1”), or else it predicts that it does not (i.e., it belongs to the negative class, labeled “0”). This makes it a binary classifier.\n",
    "\n",
    "Let’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that contains the sepal\n",
    "and petal length and width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, and\n",
    "Iris-Virginica.\n",
    "![image](https://cdn-images-1.medium.com/max/720/1*7bnLKsChXq94QjtAiRn40w.png)\n",
    "![image](http://www.robosoup.com/wp-content/uploads/2016/02/iris_petal_sepal-278x300.png)\n",
    "\n",
    "Iris data is already present in the sklearn library, and is essentially the same as the one you used during your first weekly exercise. All we need to do is to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# iris is a dictionary of key-value pairs. Each key-value pairs contains some information about the dataset.\n",
    "# Lets display a list of these keys and see what they hold\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `data`: holds the data of sepal and petal lengths and widths in four columns,\n",
    "- `target`: holds the class of each flower. These class are encoded as 0, 1, and 2,\n",
    "- `target_names`: holds the names of each of the flower classes,\n",
    "- `DESCR`: contains a detailed description of the dataset, and\n",
    "- `feature_names`: contains a list of name of the columns of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us get the petal width. It is present in the 4th column of data\n",
    "X = iris[\"data\"][:, 3:] \n",
    "X.sort(axis=0)\n",
    "\n",
    "# lets define a binaray variable that encodes whether a flower is Iris-Virginca or not\n",
    "# Iris_virginca flower is encoded as a 2 in target  \n",
    "y = (iris[\"target\"] == 2).astype(np.int) # 1 if Iris-Virginica, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.xlabel('Petal width (cm)')\n",
    "plt.ylabel('Iris-Virginica(1) \\n Not Iris-Virginica(0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first try the naive thing of fitting a linear model to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "y_ = lin_reg.predict(X)\n",
    "\n",
    "plt.scatter(X, y, label='original data')\n",
    "plt.plot(X, y_, color='r', label='fit from linear regression')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression model cannot be an optimal fit for such dichotomous data. Lets try now to fit a logistic regression model to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now look at the model’s estimated probabilities for flowers with petal widths varying from 0 to 3 cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate X_new which is vector of closely spaced points form 0 to 3\n",
    "# This vector will help us plot the model\n",
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "\n",
    "# make a vector of prediction probablity values for all datapoints in X_new\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica Prob\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica Prob\")\n",
    "plt.scatter(X, y, label='data')\n",
    "\n",
    "plt.xlabel('Petal width (cm)')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The petal width of Iris-Virginica flowers ranges from 1.4 cm to 2.5 cm, while the other iris flowers generally have a smaller petal width, ranging from 0.1 cm to 1.8 cm. \n",
    "\n",
    "Notice that there is a bit of overlap. Above about 2 cm the classifier is highly confident that the flower is an Iris-Virginica (it outputs a high probability to that class), while below 1 cm it is highly confident that it is not an Iris-Virginica (high probability for the “Not Iris-Virginica” class). In between these extremes, the classifier is unsure. However, if you ask it to predict the class (using the `predict()` method rather than the `predict_proba()` method), it will return whichever class is the most likely. Therefore, there is a decision boundary at around 1.6 cm where both probabilities are equal to 0.5 (or 50%): if the petal width is higher than 1.6 cm, the classifier will predict that the flower is an Iris-Virginica, or else it will predict that it is not (even if it is not very confident).\n",
    "\n",
    "Let's try to predict the class of a flower that has a petal width of 1.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict([[1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the class predicted is 1 or Iris-Virginica. Let us find out how sure the classifier was while making this decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict_proba([[1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the classifier was about 56% sure that the flower is Iris-Virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1<a class=\"anchor\" id=\"exercise3_1\"></a> \n",
    "In the example above, we discussed a two class classification problem i.e, we built a logistic regression classifier that was able to distinguish between Iris-Virginica and non-Iris-Virginica flowers based on just a single feature: the petal width.\n",
    "\n",
    "Your task now is build a multi-class classifier that can distinguish between Iris-Virginca, Iris-Setosa, and Iris-Versicolor. Furthermore, instead of using one feature, now you have to use two features — petal length and petal width — to train your model.\n",
    "\n",
    "Using the model that you trained, predict the most probable class for a flower that has petal length and width of 1 and 0.1 cm, respectively. What is probability value of this most probable class.\n",
    "\n",
    "For those who prefer to work in R, you can use the `glm()` function to perform logistic regression. More on it [here](https://www.datacamp.com/community/tutorials/logistic-regression-R). \n",
    "\n",
    "##### Hint:\n",
    "1. Make an object from LogisticRegression class in sklearn as following: \n",
    "\n",
    "`multiclass_logreg_obj = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)`.\n",
    "\n",
    "`multiclass_logreg_obj` is just a name. It could be any (appropriate) name you like.\n",
    "\n",
    "Read more about the Logistic Regression parameters in the [online documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Understanding C, solver, and multi_class is important for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources \n",
    "1. https://github.com/mattnedrich/GradientDescentExample/blob/master/gradient_descent_example.py\n",
    "2. Hands-On Machine Learning with Scikit-Learn and TensorFlow by Aurélien Géron\n",
    "\n",
    "For further details on linear regression, we recommend watching the Linear Regression lectures in the Machine Learning Course by Andrew Ng on Coursera."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
