{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAtApKyPZnzL"
   },
   "source": [
    "\n",
    "# INF283 | Weekly Exercise 06 | Bayesian inference\n",
    "\n",
    "#### Deadline\n",
    "October 19, 2018 | 23:59\n",
    "\n",
    "#### What to deliver \n",
    "You can try out your Python code within this notebook. But you should <font color=\"#b51555\">make a PDF file of answers for each of the tasks, and then submit this PDF file </font>on Mitt UiB.  <font color=\"#b51555\">**Do not submit the complete notebook; it makes it difficult for the graders to locate your answers in the lengthy Jupyter notebook. **</font>\n",
    "\n",
    "#### Where to deliver\n",
    "On [Mitt UiB](https://mitt.uib.no/courses/12791/assignments) in the assignments section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pifOqV7y8RkV"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "## <font color=\"#b51555\">Exercise 1.1\n",
    "As an avid player of board games you have a nice collection of non-standard\n",
    "dice:  You  have  a  3-sided,  5-sided,  7-sided,  11-sided  and  20-sided  die.   The  5  dice  are  treasured  in  a\n",
    "beautiful purple velvet bag.  \n",
    "  \n",
    "Without looking, a friend of yours randomly chooses a die from the bag and rolls a 6.\n",
    "\n",
    "1.   What is the probability that the 11-sided die was chosen? \n",
    "2.   What is the probability that the 20-sided die was used for the role?  Show your work!\n",
    "\n",
    "Now your friend rolls (with the same die!) an 18.  \n",
    "\n",
    "3.  What is the probability now that the die is 11-sided?\n",
    "4.  What is the probability that it is 20-sided?  Show your work!\n",
    "\n",
    "<font>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zV1iQ8fElCy0"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "## <font color=\"#b51555\">Exercise 1.2\n",
    "Let us consider a coin tossing experiment as a **Bernoulli model**. Let us assume the probability of getting a head in a trial, i.e., $P(H) = Œ∏$. ($T$ denotes getting a tail).\n",
    "  \n",
    "**(a)** Calculate the probability of getting exactly 9 heads when a fair coin is tossed 15 times.\n",
    "  \n",
    "**(b)** Now assume the outcome of the above experiment is D = {HHTHHTHTTHHTTHH}.\n",
    "  \n",
    "b1.  Calculate the maximum likelihood parameters.\n",
    "  \n",
    "b2.  Calculate the likelihood of the data with these parameters.\n",
    "\n",
    "b3.  Calculate the likelihood when the coin is fair.\n",
    "\n",
    "b4.  Calculate posterior distribution, i.e., P(Œ∏ | D) considering a uniform prior.\n",
    "\n",
    "b5.  Now in the given order, for each observation X, calculate its predictive probability given the preceding sequence, i.e., (in this case) calculate $P(X = H), P(X = H | D = {H}), P(X = T | D = {HH}), P(X = H | D = {HHT}), ùëÉ(ùëã=H|ùê∑=ùêªHTH), and ùëÉ(ùëã=T|ùê∑=ùêªHTHH)$ using maximum likelihood parameters and Bayesian inference (all parameters) with the uniform prior. \n",
    "\n",
    "**(c)** Calculate all of (b) when the outcome of the experiment is D = {TTTHHHHHHH}.\n",
    "\n",
    "\n",
    "<font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJu35AJbKxFP"
   },
   "source": [
    "## Naive Bayes Classification\n",
    "\n",
    "Consider the following data which shows if tennis was played on a Saturday based on weather, temperature, humdity, and wind conditions of the day:\n",
    "\n",
    "| Day # | Outlook  | Temperature | Humidity | Wind  | PlayTennis |\n",
    "|-------|----------|-------------|----------|-------|------------|\n",
    "| 0     | Rainy    | Hot         | High     | False | Yes         |\n",
    "| 1     | Rainy    | Hot         | High     | True  | No         |\n",
    "| 2     | Overcast | Hot         | High     | False | Yes        |\n",
    "| 3     | Sunny    | Mild        | High     | False | Yes        |\n",
    "| 4     | Sunny    | Cool        | Normal   | False | No        |\n",
    "| 5     | Sunny    | Cool        | Normal   | True  | No         |\n",
    "| 6     | Overcast | Cool        | Normal   | True  | Yes        |\n",
    "| 7     | Rainy    | Mild        | High     | False | No         |\n",
    "| 8     | Rainy    | Cool        | Normal   | False | Yes        |\n",
    "| 9     | Sunny    | Mild        | Normal   | False | Yes        |\n",
    "| 10    | Rainy    | Mild        | Normal   | True  | Yes        |\n",
    "| 11    | Overcast | Mild        | High     | True  | Yes        |\n",
    "| 12    | Overcast | Hot         | Normal   | False | Yes        |\n",
    "| 13    | Sunny    | Mild        | High     | True  | No         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrFpH0btV8Y5"
   },
   "source": [
    "---\n",
    "\n",
    "## <font color=\"#b51555\">Exercise 1.3\n",
    "Suppose that the forecast for the coming Saturday is:\n",
    "*   weather: Sunny\n",
    "*   temperature: Hot\n",
    "*   humidity: Normal\n",
    "*   wind: False\n",
    "\n",
    "Based on the data for the 14 previous Saturdays, how likely it is that tennis will be played the coming Saturday?\n",
    "\n",
    "**Hint:** \n",
    "  \n",
    "To calculate, lets say P(Sunny | Yes) and P(Sunny | No), we first make a table of **Outlook** and **PlayTennis** entries, as shown below:\n",
    "  \n",
    "  | Day # | Outlook  | PlayTennis |\n",
    "|-------|----------|------------|\n",
    "| 0     | Rainy     | Yes         |\n",
    "| 1     | Rainy     | No         |\n",
    "| 2     | Overcast | Yes        |\n",
    "| 3     | Sunny    | Yes        |\n",
    "| 4     | Sunny    | No        |\n",
    "| 5     | Sunny    | No         |\n",
    "| 6     | Overcast  | Yes        |\n",
    "| 7     | Rainy     | No         |\n",
    "| 8     | Rainy    | Yes        |\n",
    "| 9     | Sunny     | Yes        |\n",
    "| 10    | Rainy      | Yes        |\n",
    "| 11    | Overcast  | Yes        |\n",
    "| 12    | Overcast  | Yes        |\n",
    "| 13    | Sunny      | No         |\n",
    "  \n",
    "We can compress this table to:\n",
    "  \n",
    "| |  | OUTLOOK |\n",
    "  |------------|-------|-------|----------|-------|\n",
    "|  | Rainy | Sunny | Overcast | Total |\n",
    "| PlayTennis = Yes        | 3     | 2     | 4        | **9**     |\n",
    "| PlayTennis =No         | 2     | 3    | 0        | **5**     |\n",
    "| Total      |       |       |          | **14**   |\n",
    "  \n",
    "Now it is easy to calculate the probabilities. For example, P(Sunny | Yes)  is 2/9 and P(Sunny | No) is 3/5 and so on.\n",
    "  \n",
    "<font>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENgopOfDXMJ4"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "## <font color=\"#b51555\">Exercise 1.4\n",
    "Suppose that the forecast for the coming Saturday is:\n",
    "*   weather: Sunny\n",
    "*   temperature: Hot\n",
    "*   humidity: unknown\n",
    "*   wind: unknown\n",
    "\n",
    "Based on the data for the 14 previous Saturdays and the partial data for the coming saturday (above) how likely it is that tennis will be played coming Saturday?\n",
    "<font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJrFX2dh_Zwv"
   },
   "source": [
    "## Bayesian Regression\n",
    "In exercise 02, we demonstrated that ridge regression (a form of regularized linear regression that attempts to shrink the learned weights toward zero) can be very effective at combating overfitting and lead to a greatly more generalizable model. The amount of shrinking depended on value of the regularization parameter, which is a hyperparameter. If we selected the wrong value for this hyperparameter, our fit could be sub-optimal. \n",
    "\n",
    "What if we could get the same results as obtained by an optimal ridge regression, but without having to go thorough the selection of a proper value for the regularization parameter. That's where Bayesian regression comes into the picture. Bayesian regression will yeild the same results as an optimal ridge regression without us having to set any hyperparameters.\n",
    "\n",
    "The weights found by the Bayesian regression would be more or less the same as the weights found by the optimal ridge regession. Having said that, both approaches are based on two very different techniques. The downside of using the Bayesian approach is that it is slower than the ridge regression.\n",
    "\n",
    "In the example below,  we will create a vector `y` from a linear combination of columns of `X`, where `X` is 100 x 100 matrix, so it has 100 features (columns) and 100 instances (rows). While generating `y` from `X`, we will set random weights for 10 randomly selected features of `X`, while the remaining 90 features of `X` will have their weights set to zero. We do this so that we can assess Bayesian and Ridge regression: If they are working correctly, then they should correctly estimate the value of the 10 weights that we have used while creating `y`, and they should set the remaining 90 weights to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BcWoEsnY_dqg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Ridge, BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5x3YFyiKAMqv"
   },
   "outputs": [],
   "source": [
    "# Generating simulated data with Gaussian weights\n",
    "np.random.seed(0)\n",
    "n_samples, n_features = 100, 100\n",
    "X = np.random.randn(n_samples, n_features)  # Create Gaussian data\n",
    "# Create weights with a precision alpha_ of 4.\n",
    "alpha_ = 4.\n",
    "w = np.zeros(n_features)\n",
    "# Only keep 10 weights of interest\n",
    "relevant_features = np.random.randint(0, n_features, 10)\n",
    "for i in relevant_features:\n",
    "    # Set weights for these features of interest by generating a random sample\n",
    "    # from a normal random variable with mean 0 and standard deviation 1\n",
    "    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_))\n",
    "# Create noise with a precision beta of 50.\n",
    "beta_ = 50.\n",
    "noise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(beta_), size=n_samples)\n",
    "# Create the target\n",
    "y = np.dot(X, w) + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIBFd-LYgJd-"
   },
   "source": [
    "Now we fit a Bayesian and Ridge regression to the data. For Ridge regression, we set the regularization parameter to 1, which gives us an optimal fit for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRusQq_mAQSi"
   },
   "outputs": [],
   "source": [
    "# Fit the Bayesian Ridge Regression \n",
    "bay = BayesianRidge(compute_score=True)\n",
    "bay.fit(X, y)\n",
    "\n",
    "# And fit Ridge regression for comparison\n",
    "rdg = Ridge(alpha = 1)\n",
    "rdg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V-pZXw0Ngc5Z"
   },
   "source": [
    "Now we plot the ground-truth weigths, and the weights found by Bayesian and Ridge regresssions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgNFDgoMAWZu"
   },
   "outputs": [],
   "source": [
    "lw = 2\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.title(\"Weights of the model\")\n",
    "\n",
    "# plot the weights found by Bayesian Ridge regression\n",
    "plt.plot(bay.coef_, color='red', linewidth=lw,\n",
    "         label=\"Bayesian Ridge estimate\")\n",
    "\n",
    "# plot weights of Ridge regression\n",
    "plt.plot(rdg.coef_, color='navy', linestyle='--', label=\"Ridge estimate\")\n",
    "\n",
    "# plot the actual ground-truth weights\n",
    "plt.plot(w, color='green', linewidth=lw, label=\"Ground truth\")\n",
    "\n",
    "# label axes\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Values of the weights\")\n",
    "plt.legend(loc=\"best\", prop=dict(size=12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VypFehIbgvBx"
   },
   "source": [
    "We observe that both Bayesian and ridge regression estimate the same values for all the weights. They both also pull the estimates of weights that are zero in the ground-truth, towards zero, thereby regularizing the fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHVCuM-1w7cm"
   },
   "source": [
    "---\n",
    "\n",
    "## <font color=\"#b51555\">Exercise 1.5\n",
    "For this exercise, consider the code below in which we fitted both ridge and bayesian regressions to polynomial features of degree 15. We then poltted the fits predicted by both the classifiers alongwith the original data.\n",
    "\n",
    "The predict function in the Bayesian regression class also has an addition argument `return_std`. If this parameter is set to True, then the classifier will not only give predicitions but also the uncertainty estimates as standard deviation (centered at the predicted value) centered around the mean value.\n",
    "  \n",
    " Your job is modify the code below so that you obtain a plot like this:\n",
    "  \n",
    " ![image](https://i.postimg.cc/VkmkSQL1/plot.png) \n",
    " \n",
    " The plot has uncertanity estimates from the Bayesian regression prediction as error bars around the predicted values.\n",
    "<font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qiadhOBVFqMe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import Ridge, BayesianRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# define the number of points to generate\n",
    "k = 100\n",
    "noise_factor = 1\n",
    "np.random.seed(10)\n",
    "\n",
    "# generate k x-axis values from 0 to 10\n",
    "X = np.linspace(0, 10, k)\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "# generate k y-axis values (with noise)\n",
    "y = np.sqrt(X) * np.sin(X)\n",
    "noise = np.random.normal(0, 1, k).reshape(X.shape)\n",
    "y = y + noise_factor*noise\n",
    "\n",
    "# Create polynomial feature (degree 15)\n",
    "poly_features = PolynomialFeatures(degree=15, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "# Create Ridge regression object from Ridge class\n",
    "ridge_reg = Ridge(alpha=800)\n",
    "# Fit data using Ridge regression\n",
    "ridge_reg.fit(X_poly, y)\n",
    "\n",
    "# Create a Bayesian regression object from BayesianRidge class\n",
    "bayesian_reg = BayesianRidge(compute_score=True)\n",
    "# Fit data using Bayesian regression\n",
    "bayesian_reg.fit(X_poly, y)\n",
    "\n",
    "y_predict_ridge = ridge_reg.predict(X_poly)\n",
    "y_predict_bayesian = bayesian_reg.predict(X_poly)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.plot(X, y_predict_bayesian, color='navy', label='Bayesian regression fit')\n",
    "plt.plot(X, y_predict_ridge, color='r', label='Ridge regression fit')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmGtFdavYs5I"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Sources and Attributions:\n",
    "1. Python Data Science Handbook by Jake VanderPlas\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "INF-283 Weekly Exercise 06-sv.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
